# ğŸ§  ContextIQ â€” Local RAG Assistant

A professional, fully local **Retrieval-Augmented Generation (RAG)** system built from scratch in Python â€” no LangChain, no paid APIs.

Upload PDFs and chat with your documents using **FAISS + SentenceTransformers + Ollama (phi3:mini)** â€” fast, private, and free.

---

## âœ¨ Features

* ğŸ“‚ Upload and process multiple PDFs
* ğŸ” Semantic search with FAISS
* ğŸ§  Local embeddings (MiniLM)
* ğŸ¯ Cross-encoder reranking for better accuracy
* ğŸ’¬ Modern Streamlit chat interface
* ğŸ”’ 100% local â€” no API costs
* âš¡ Cached models for faster reloads
* ğŸ§© Clean modular pipeline

---

## ğŸ—ï¸ System Architecture

```text
Documents â†’ Chunking â†’ Embeddings â†’ FAISS â†’ Retrieval â†’ Reranking â†’ Local LLM â†’ Answer
```

### How it works

1. **Ingestion** â€” Extract text from PDFs
2. **Chunking** â€” Split into semantic chunks
3. **Embedding** â€” Convert text into vectors
4. **Vector Search** â€” Retrieve relevant chunks
5. **Reranking** â€” Improve relevance
6. **Generation** â€” Local LLM produces grounded answer

---

## ğŸ§° Tech Stack

* **Python**
* **Streamlit**
* **Sentence Transformers**
* **FAISS**
* **Ollama (phi3:mini)**
* **pdfplumber**
* **PyTorch**

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/YOUR_USERNAME/YOUR_REPO.git
cd YOUR_REPO
```

---

### 2ï¸âƒ£ Create virtual environment

```bash
python -m venv .venv
.venv\\Scripts\\activate   # Windows
# source .venv/bin/activate  # Mac/Linux
```

---

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

If you don't have it yet:

```bash
pip install streamlit pdfplumber sentence-transformers faiss-cpu numpy requests torch
```

---

### 4ï¸âƒ£ Install Ollama (Required)

Download and install from:

ğŸ‘‰ https://ollama.com/download

Pull the local model:

```bash
ollama pull phi3:mini
```

Verify installation:

```bash
ollama list
```

---

### 5ï¸âƒ£ Run the application

```bash
streamlit run rag_gradio_ui.py
```

Open the local URL shown in the terminal.

---

## ğŸ“– Usage

1. Upload PDFs from the sidebar
2. Click **Build Index**
3. Ask questions in the chat
4. Get answers grounded in your documents

---

## âš¡ Performance Notes

* First startup is slower due to model loading
* Subsequent runs are faster (cached)
* Works fully offline after models are downloaded
* CPU-only systems may have slower inference

---

## ğŸš§ Current Limitations

* Scanned/image PDFs are not supported
* Very large document sets may slow indexing
* Hybrid search not implemented yet

---

## ğŸ”® Future Improvements

* Hybrid search (BM25 + vector)
* Streaming responses
* Source citations panel
* Dark/light theme toggle
* Docker deployment
* Multi-user support

---

## ğŸ“ Recommended Project Structure

```
rag-project/
â”‚
â”œâ”€â”€ rag_gradio_ui.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â””â”€â”€ data/ (optional)
```

---

## ğŸ‘¨â€ğŸ’» Author

**Hisham Hidayathulla**

* GitHub: https://github.com/hishaaaam
* LinkedIn: https://www.linkedin.com/in/hisham-hidaya/

---

## â­ Support

If you found this useful, consider starring the repository!

---
